{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas : Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational”or “labelled” data both easy and intuitive.\n",
    "# Matplotlib & Seaborn : These packages provide a high-level interface\n",
    "# BeautifulSoup : BeautifulSoup is  a Python library. It is used for parsing XML and HTML.\n",
    "# importing pandas package\n",
    "import pandas as pd\n",
    "# importing numpy package\n",
    "import numpy as np\n",
    "# importing matplotlib and seaborn packages\n",
    "import matplotlib.pyplot as plt\n",
    "# importing re package\n",
    "import re\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "# importing NLTK package's\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "# importing gensim package's\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mobile review data\n",
    "df = pd.read_csv('training-dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 2 - Cleansing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform the cleaning of data using  wordCloud corpus\n",
    "def clean_text(raw_text, remove_stopwords=False, stemming=False, split_text=False):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case\n",
    "\n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    if stemming is True: # stemming\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    if split_text is True:  # split text\n",
    "        return words\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform tokenization\n",
    "def parse_sent(text, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) is not 0:\n",
    "            sentences.append(clean_text(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the data\n",
    "print(\"Summary statistics of numerical features : \\n\", df.describe())\n",
    "# Total number of reviews\n",
    "print(\"\\nTotal number of reviews: \", len(df))\n",
    "# Percentage of positive, negative and Neutral reviews\n",
    "print(\"\\nPercentage of reviews with neutral sentiment : {:.2f}%\"      \\\n",
    "      .format(df[df['rating'] == 3][\"body\"].count()/len(df)*100))\n",
    "print(\"\\nPercentage of reviews with positive sentiment : {:.2f}%\"     \\\n",
    "      .format(df[df['rating'] > 3][\"body\"].count()/len(df)*100))\n",
    "print(\"\\nPercentage of reviews with negative sentiment : {:.2f}%\"     \\\n",
    "      .format(df[df['rating'] < 3][\"body\"].count()/len(df)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing Bar Plot on distribution of rating\n",
    "plt.figure(figsize=(12, 8))\n",
    "df['rating'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing Bar Plot on distribution of review length\n",
    "REVIEW_LENGTH = df[\"body\"].dropna().map(lambda x: len(x))\n",
    "plt.figure(figsize=(12, 8))\n",
    "REVIEW_LENGTH.loc[REVIEW_LENGTH < 1500].hist()\n",
    "plt.title(\"Distribution of Review Length\")\n",
    "plt.xlabel('Review length (Number of character)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-4 - Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in the data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in the data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 4s and 5s as 1 (positive sentiment) and 1s, 2s and 3s as 0 (negative sentiment)\n",
    "df['sentiment'] = np.where(df['rating'] > 3, 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sentiment', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[['body', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = []\n",
    "# final sentence after performing cleansing\n",
    "for d in train_df['body']:\n",
    "    train_clean.append(clean_text(d))\n",
    "print('Show a cleaned review in the training set : \\n', train_clean[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
    "# nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse each review in the training set into sentences\n",
    "SENTENCES = []\n",
    "for review in train_clean:\n",
    "    SENTENCES += parse_sent(review, TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%d parsed sentence in the training set\\n'  %len(SENTENCES))\n",
    "print('Show a parsed sentence in the training set : \\n', SENTENCES[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 300  #embedding dimension\n",
    "MIN_WORD_COUNT = 5\n",
    "NUM_WORKERS = 4\n",
    "CONTEXT = 5\n",
    "DOWNSAMPLING = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model ...\\n\")\n",
    "# perform word2vec\n",
    "W2V = Word2Vec(SENTENCES, workers=NUM_WORKERS, vector_size=NUM_FEATURES, \\\n",
    "          min_count=MIN_WORD_COUNT, window=CONTEXT, sample=DOWNSAMPLING)\n",
    "W2V.init_sims(replace=True)\n",
    "W2V.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(W2V.wv.index_to_key))\n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", W2V.wv.index_to_key[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain features from the word2vec\n",
    "def make_feature_vec(text, model, num_feat):\n",
    "    '''\n",
    "    Transform a review to a feature vector by averaging feature vectors of words\n",
    "    appeared in that review and in the volcabulary list created\n",
    "    '''\n",
    "    feat_vector = np.zeros((num_feat,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key) # index2word is the volcabulary list of the Word2Vec model\n",
    "    zero_vector = True\n",
    "    for word in text:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            feat_vector = np.add(feat_vector, model.wv[word])\n",
    "            zero_vector = False\n",
    "    if zero_vector is False:\n",
    "        feat_vector = np.divide(feat_vector, nwords)\n",
    "    return feat_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain average features from the sentance\n",
    "def get_avg_feature_vecs(texts, model, num_feat):\n",
    "    '''\n",
    "    Transform all reviews to feature vectors using make_feature_vec()\n",
    "    '''\n",
    "    counter = 0\n",
    "    review_feature_vectors = np.zeros((len(texts), num_feat), dtype=\"float32\")\n",
    "    for text in texts:\n",
    "        review_feature_vectors[counter] = make_feature_vec(text, model, num_feat)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vectors for training set\n",
    "X_TRAIN_CLEANED = []\n",
    "for review in train_df['body']:\n",
    "    X_TRAIN_CLEANED.append(clean_text(review, remove_stopwords=True, split_text=True))\n",
    "TRAIN_VECTOR = get_avg_feature_vecs(X_TRAIN_CLEANED, W2V, NUM_FEATURES)\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %TRAIN_VECTOR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_df['body']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Applying Algorithm and Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(TRAIN_VECTOR, train_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 6 - Preparing Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mobile review test data\n",
    "test_df = pd.read_csv('testing-dataset.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 4s and 5s as 1 (positive sentiment) and 1s, 2s and 3s as 0 (negative sentiment)\n",
    "test_df['sentiment'] = np.where(test_df['rating'] > 3, 1, 0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = []\n",
    "for d in test_df['body']:\n",
    "    test_clean.append(clean_text(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vectors for validation set\n",
    "X_TEST_CLEANED = []\n",
    "for review in test_df['body']:\n",
    "    X_TEST_CLEANED.append(clean_text(review, remove_stopwords=True, split_text=True))\n",
    "TEST_VECTOR = get_avg_feature_vecs(X_TEST_CLEANED, W2V, NUM_FEATURES)\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %TEST_VECTOR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Scoring the model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = RF.predict(TEST_VECTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 - Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate the model\n",
    "def model_evaluation(predict):\n",
    "    '''\n",
    "    Print model evaluation to predicted result\n",
    "    '''\n",
    "    print(\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(test_df['sentiment'], predict)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(test_df['sentiment'], predict)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(test_df['sentiment'], predict))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(test_df['sentiment'], predict))\n",
    "\n",
    "# to predict the model\n",
    "model_evaluation(PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFUSION_MATRIX = metrics.confusion_matrix(test_df['sentiment'], PREDICTIONS)\n",
    "sns.heatmap(CONFUSION_MATRIX, annot=True, fmt='g', xticklabels=\\\n",
    "              [\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate the model\n",
    "def model_evaluation(predict):\n",
    "    '''\n",
    "    Print model evaluation to predicted result\n",
    "    '''\n",
    "    print(\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(test_df['sentiment'], predict)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(test_df['sentiment'], predict)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(test_df['sentiment'], predict))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(test_df['sentiment'], predict))\n",
    "\n",
    "# to predict the model\n",
    "model_evaluation(PREDICTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
